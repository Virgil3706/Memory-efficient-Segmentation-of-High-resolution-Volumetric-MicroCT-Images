{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "meU-net.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "R0bc9C9jc730",
        "Cbh7futKDK4O"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47pkFsr_dAi8"
      },
      "source": [
        "#Start"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_b2ZynhvWrH"
      },
      "source": [
        "import os\n",
        "current_path = os.getcwd()\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIoESpQZ1BTN"
      },
      "source": [
        "import os\n",
        "current_path = os.getcwd()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install simpleITK==1.2.4;\n",
        "!pip install -q \"monai-weekly[gdown, nibabel, tqdm, itk]\"\n",
        "import monai\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report\n",
        "from monai.apps import download_and_extract\n",
        "from monai.config import print_config\n",
        "from monai.metrics import ROCAUCMetric\n",
        "from monai.networks.nets import DenseNet121\n",
        "from monai.transforms import AddChannel, Compose, RandAffine, RandRotate90, RandFlip, apply_transform, ToTensor\n",
        "from monai.data import Dataset, DataLoader\n",
        "from monai.utils import set_determinism\n",
        "from monai.transforms import \\\n",
        "    LoadImageD, EnsureChannelFirstD, AddChannelD, ScaleIntensityD, ToTensorD, Compose, \\\n",
        "    AsDiscreteD, SpacingD, OrientationD, ResizeD, RandAffineD\n",
        "KEYS = (\"img\", \"seg\")\n",
        "\n",
        "xform = Compose([\n",
        "    LoadImageD(KEYS),\n",
        "    RandAffineD(KEYS,\n",
        "                rotate_range=(np.pi/2, np.pi/2, np.pi/2),\n",
        "                # translate_range=(20, 20, 20),\n",
        "                scale_range=(0.1, 0.1, 0.1),\n",
        "                mode=('bilinear', 'nearest'),\n",
        "                prob=0.2),\n",
        "    # RandFlipD(KEYS,prob=0.5),\n",
        "    ToTensorD(KEYS)\n",
        "])\n",
        "KEYS_G = (\"img_global\", \"seg_global\")\n",
        "xform_G = Compose([\n",
        "    LoadImageD(KEYS_G),\n",
        "    RandAffineD(KEYS_G,\n",
        "                rotate_range=(np.pi/2, np.pi/2, np.pi/2),\n",
        "                # translate_range=(20, 20, 20),\n",
        "                scale_range=(0.1, 0.1, 0.1),\n",
        "                mode=('bilinear', 'nearest'),\n",
        "                prob=0.2),\n",
        "    # RandFlipD(KEYS,prob=0.5),\n",
        "    ToTensorD(KEYS_G)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhEDQY1msYeZ"
      },
      "source": [
        "#Pre-propressing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVTkM1WIL-mY"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "from ipywidgets import interact, fixed\n",
        "from IPython.display import display\n",
        "import gc\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "# Calculate parameters low and high from window and level\n",
        "def wl_to_lh(window, level):\n",
        "    low = level - window/2\n",
        "    high = level + window/2\n",
        "    return low,high\n",
        "\n",
        "def display_image(img, x=None, y=None, z=None, window=None, level=None, colormap='gray', crosshair=False,figure_name=''):\n",
        "    # Convert SimpleITK image to NumPy array\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "\n",
        "    # Get image dimensions in millimetres\n",
        "    size = img.GetSize()\n",
        "    spacing = img.GetSpacing()\n",
        "    width  = size[0] * spacing[0]\n",
        "    height = size[1] * spacing[1]\n",
        "    depth  = size[2] * spacing[2]\n",
        "\n",
        "    if x is None:\n",
        "        x = np.floor(size[0]/2).astype(int)\n",
        "    if y is None:\n",
        "        y = np.floor(size[1]/2).astype(int)\n",
        "    if z is None:\n",
        "        z = np.floor(size[2]/2).astype(int)\n",
        "\n",
        "    if window is None:\n",
        "        window = np.max(img_array) - np.min(img_array)\n",
        "\n",
        "    if level is None:\n",
        "        level = window / 2 + np.min(img_array)\n",
        "\n",
        "    low,high = wl_to_lh(window,level)\n",
        "\n",
        "    # Display the orthogonal slices\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
        "\n",
        "    ax1.imshow(img_array[z,:,:], cmap=colormap, clim=(low, high), extent=(0, width, height, 0))\n",
        "    ax2.imshow(img_array[:,y,:], origin='lower', cmap=colormap, clim=(low, high), extent=(0, width,  0, depth))\n",
        "    ax3.imshow(img_array[:,:,x], origin='lower', cmap=colormap, clim=(low, high), extent=(0, height, 0, depth))\n",
        "\n",
        "    # Additionally display crosshairs\n",
        "    if crosshair:\n",
        "        ax1.axhline(y * spacing[1], lw=1)\n",
        "        ax1.axvline(x * spacing[0], lw=1)\n",
        "        ax2.axhline(z * spacing[2], lw=1)\n",
        "        ax2.axvline(x * spacing[0], lw=1)\n",
        "        ax3.axhline(z * spacing[2], lw=1)\n",
        "        ax3.axvline(y * spacing[1], lw=1)\n",
        "    if figure_name != '':\n",
        "      plt.savefig(os.path.join(figure_dir,str(figure_name)+'.jpg'))\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def interactive_view(img):\n",
        "    size = img.GetSize()\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "    interact(display_image,img=fixed(img),\n",
        "             x=(0, size[0] - 1),\n",
        "             y=(0, size[1] - 1),\n",
        "             z=(0, size[2] - 1),\n",
        "             window=(0,np.max(img_array) - np.min(img_array)),\n",
        "             level=(np.min(img_array),np.max(img_array)));\n",
        "\n",
        "def make_one_hot(vol, mask):\n",
        "    lens = len(mask)\n",
        "    shape = np.array(vol.shape)\n",
        "    shape[1] = lens\n",
        "    shape = tuple(shape)\n",
        "    result = torch.zeros(shape,dtype = vol.dtype).to(vol.device)\n",
        "\n",
        "    for idx, label in enumerate(mask):\n",
        "        tmp = vol == label\n",
        "        result[:,idx] = tmp.squeeze(1)\n",
        "\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSQUOgl3rBLj"
      },
      "source": [
        "def zero_mean_unit_var(image, mask):\n",
        "    \"\"\"Normalizes an image to zero mean and unit variance.\"\"\"\n",
        "\n",
        "    img_array = sitk.GetArrayFromImage(image)\n",
        "    img_array = img_array.astype(np.float32)\n",
        "\n",
        "    msk_array = sitk.GetArrayFromImage(mask)\n",
        "\n",
        "    mean = np.mean(img_array[msk_array>0])\n",
        "    std = np.std(img_array[msk_array>0])\n",
        "\n",
        "    if std > 0:\n",
        "        img_array = (img_array - mean) / std\n",
        "        img_array[msk_array==0] = 0\n",
        "\n",
        "    image_normalised = sitk.GetImageFromArray(img_array)\n",
        "    image_normalised.CopyInformation(image)\n",
        "\n",
        "    return image_normalised\n",
        "\n",
        "\n",
        "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
        "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
        "\n",
        "    original_spacing = np.array(image.GetSpacing())\n",
        "    original_size = np.array(image.GetSize())\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
        "    else:\n",
        "        out_size = np.array(out_size)\n",
        "\n",
        "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
        "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
        "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
        "\n",
        "    original_center = np.matmul(original_direction, original_center)\n",
        "    out_center = np.matmul(original_direction, out_center)\n",
        "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
        "\n",
        "    resample = sitk.ResampleImageFilter()\n",
        "    resample.SetOutputSpacing(out_spacing)\n",
        "    resample.SetSize(out_size.tolist())\n",
        "    resample.SetOutputDirection(image.GetDirection())\n",
        "    resample.SetOutputOrigin(out_origin.tolist())\n",
        "    resample.SetTransform(sitk.Transform())\n",
        "    resample.SetDefaultPixelValue(pad_value)\n",
        "\n",
        "    if is_label:\n",
        "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "    else:\n",
        "        resample.SetInterpolator(sitk.sitkBSpline)\n",
        "    import os\n",
        "    import sys\n",
        "    import psutil\n",
        "\n",
        "    del original_size\n",
        "\n",
        "    gc.collect()\n",
        "    return resample.Execute(image)\n",
        "\n",
        "def prepadding(data,kernel_c,kernel_h,kernel_w,step):\n",
        "    #############input size:\n",
        "    ####################batch_size, depth(n_channels), n_rows, n_cols\n",
        "    import torch\n",
        "    import torch.nn\n",
        "    batch_size, depth, height, width = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unsqueeze(1)\n",
        "\n",
        "    # calculate the padding for two edges\n",
        "    def cal_padding(n,k,stride,p):\n",
        "        #the two boundry: left  right\n",
        "        total_padding = stride - (n - (k+(int((n-k+2*p)/stride))*stride))\n",
        "        if total_padding == stride:\n",
        "          total_padding = 0\n",
        "        left = int(total_padding/2)\n",
        "        right = total_padding - left\n",
        "        return left,right\n",
        "\n",
        "    padding_left,padding_right = cal_padding(n=width,k=kernel_w,stride=step,p=0)\n",
        "    padding_top,padding_bottom = cal_padding(n=height,k=kernel_h,stride=step,p=0)\n",
        "    padding_front,padding_back = cal_padding(n=depth,k=kernel_c,stride=step,p=0)\n",
        "\n",
        "    output = torch.zeros((batch_size,depth+padding_front+padding_back,height+padding_top+padding_bottom,width+padding_left+padding_right),dtype = data.dtype).to(device)\n",
        "    output[:,padding_front:depth+padding_front,padding_top:height+padding_top,padding_left:width+padding_left]= data\n",
        "    print(output.shape)\n",
        "    del data\n",
        "    gc.collect()\n",
        "    return output\n",
        "\n",
        "\n",
        "\n",
        "def get_patches(data,kernel_c=32, kernel_h=32, kernel_w=32,step=32):\n",
        "    data = prepadding(data,kernel_c,kernel_h,kernel_w,step)\n",
        "    gc.collect()\n",
        "    batch_size, n_channels, n_rows, n_cols = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unfold(1, kernel_c, step).unfold(2, kernel_h, step).unfold(3, kernel_w, step)\n",
        "    data = data.permute(1, 2, 3, 0, 4, 5, 6).reshape(-1, kernel_c, kernel_h, kernel_w)\n",
        "    \n",
        "    return data\n",
        "\n",
        "def get_patches_and_bound(data,kernel_c=32, kernel_h=32, kernel_w=32,step=32):\n",
        "    data = prepadding(data,kernel_c,kernel_h,kernel_w,step)\n",
        "    gc.collect()\n",
        "    batch_size, n_channels, n_rows, n_cols = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unfold(1, kernel_c, step).unfold(2, kernel_h, step).unfold(3, kernel_w, step)\n",
        "    shapes = data.shape[0:4]\n",
        "    data = data.permute(1, 2, 3, 0, 4, 5, 6).reshape(-1, kernel_c, kernel_h, kernel_w)\n",
        "    \n",
        "    return data,shapes\n",
        "\n",
        "#find patch locations\n",
        "def find_location(index_patches,shape1,step):\n",
        "    count = 0\n",
        "    for b in range(1):\n",
        "        for c in range(shape1[1]):\n",
        "            for h in range(shape1[2]):\n",
        "                for w in range(shape1[3]):\n",
        "\n",
        "                    if index_patches == count:\n",
        "                        ld = c*step\n",
        "                        lh = h*step\n",
        "                        lw = w*step\n",
        "                        return (ld,lh,lw)\n",
        "                    count+=1\n",
        "def crop_and_pad_2x(x,ld,lh,lw,len_d,len_h,len_w):\n",
        "\n",
        "  shape1 = x.shape \n",
        "  len_d_total,len_h_total,len_w_total =len_d*2,len_h*2, len_w*2 \n",
        "\n",
        "\n",
        "  padding_front,padding_back,padding_top,padding_bottom,padding_left,padding_right =0,0,0,0,0,0\n",
        "\n",
        "  expand_front,expand_back = int(len_d/2),len_d - int(len_d/2)\n",
        "  expand_top,expand_bottom = int(len_h/2),len_h - int(len_h/2)\n",
        "  expand_left,expand_right = int(len_w/2),len_w - int(len_w/2)\n",
        "\n",
        "  \n",
        "  ld_crop,lh_crop,lw_crop = ld - expand_front ,lh - expand_top, lw - expand_left\n",
        "  ld_crop_,lh_crop_,lw_crop_ = ld+len_d+expand_back,lh+len_h+expand_bottom,lw+len_w+expand_right\n",
        "  #\n",
        "  if ld_crop<0:\n",
        "    padding_front  = -ld_crop; ld_crop = 0\n",
        "  if lh_crop<0:\n",
        "    padding_top = -lh_crop; lh_crop = 0\n",
        "  if lw_crop<0:\n",
        "    padding_left = -lw_crop; lw_crop = 0\n",
        "  #\n",
        "  if ld_crop_>shape1[2]:\n",
        "    padding_back = ld_crop_ - shape1[2]; ld_crop_= shape1[2]\n",
        "  if lh_crop_>shape1[3]:\n",
        "    padding_bottom = lh_crop_ - shape1[3]; lh_crops_= shape1[3]\n",
        "  if lw_crop_>shape1[4]:\n",
        "    padding_right = lw_crop_ - shape1[4]; lw_crop_= shape1[4]\n",
        "\n",
        "  #crop\n",
        "  crop = x[:,:,ld_crop:ld_crop_,lh_crop:lh_crop_,lw_crop:lw_crop_]\n",
        "  #padding\n",
        "  pad_dims = (padding_left,padding_right,padding_top,padding_bottom,padding_front,padding_back)\n",
        "  return F.pad(crop,pad_dims,\"constant\")\n",
        "\n",
        "\n",
        "def crop_and_pad_1_5x(x,ld,lh,lw,len_d,len_h,len_w):\n",
        "\n",
        "  shape1 = x.shape \n",
        "  len_d_total,len_h_total,len_w_total =int(len_d*1.5),int(len_h*1.5), int(len_w*1.5) #\n",
        "\n",
        "\n",
        "  padding_front,padding_back,padding_top,padding_bottom,padding_left,padding_right =0,0,0,0,0,0\n",
        "\n",
        "  expand_front,expand_back = int(len_d/4),int(len_d/2) - int(len_d/4)\n",
        "  expand_top,expand_bottom = int(len_h/4),int(len_h/2) - int(len_h/4)\n",
        "  expand_left,expand_right = int(len_w/4),int(len_w/2) - int(len_w/4)\n",
        "\n",
        "  #\n",
        "  ld_crop,lh_crop,lw_crop = ld - expand_front ,lh - expand_top, lw - expand_left\n",
        "  ld_crop_,lh_crop_,lw_crop_ = ld+len_d+expand_back,lh+len_h+expand_bottom,lw+len_w+expand_right\n",
        "  #\n",
        "  if ld_crop<0:#\n",
        "    padding_front  = -ld_crop; ld_crop = 0\n",
        "  if lh_crop<0:\n",
        "    padding_top = -lh_crop; lh_crop = 0\n",
        "  if lw_crop<0:\n",
        "    padding_left = -lw_crop; lw_crop = 0\n",
        "  #\n",
        "  if ld_crop_>shape1[2]:#\n",
        "    padding_back = ld_crop_ - shape1[2]; ld_crop_= shape1[2]\n",
        "  if lh_crop_>shape1[3]:#\n",
        "    padding_bottom = lh_crop_ - shape1[3]; lh_crops_= shape1[3]\n",
        "  if lw_crop_>shape1[4]:#\n",
        "    padding_right = lw_crop_ - shape1[4]; lw_crop_= shape1[4]\n",
        "\n",
        "  #crop\n",
        "  crop = x[:,:,ld_crop:ld_crop_,lh_crop:lh_crop_,lw_crop:lw_crop_]\n",
        "  #padding\n",
        "  pad_dims = (padding_left,padding_right,padding_top,padding_bottom,padding_front,padding_back)\n",
        "  return F.pad(crop,pad_dims,\"constant\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKDJi0CUvEeA"
      },
      "source": [
        "### dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrTrS2gAftd5"
      },
      "source": [
        "class ImageSegmentationDataset_separate(Dataset):\n",
        "    \"\"\"Dataset for image segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, file_list_img, file_list_seg, img_spacing, img_size,\n",
        "                 patch=True, kernel_c=64, kernel_h=64, kernel_w=64, step=32, TEST=False):\n",
        "        self.imgs = []\n",
        "        self.imgs_global = []\n",
        "        self.segs = []\n",
        "        self.segs_global = []\n",
        "        self.img_names = []\n",
        "        self.seg_names = []\n",
        "        # count the number of class items in patches for weights\n",
        "        self.counts = torch.zeros(9)\n",
        "        self.test = TEST\n",
        "        self.datasetname = 'train' if not TEST else 'val_test'\n",
        "        torch.manual_seed(1928)\n",
        "        roulette_fix = list(torch.rand(50000))\n",
        " \n",
        "        with torch.no_grad():\n",
        "            for idx, _ in enumerate(tqdm(range(len(file_list_img)), desc='Loading Data')):\n",
        "                valid_index = []\n",
        "                seg_path = file_list_seg[idx]\n",
        "\n",
        "                seg = sitk.ReadImage(seg_path, sitk.sitkInt8)\n",
        "\n",
        "\n",
        "                # ##resample image\n",
        "                # seg = resample_image(seg, img_spacing, img_size, is_label=True)\n",
        "\n",
        "                # from image  numpy to torch\n",
        "                seg = torch.from_numpy(sitk.GetArrayFromImage(seg)).unsqueeze(0).to(device)\n",
        "                seg_whole = seg #原图\n",
        "                # print('shape: ',img.shape,seg.shape)\n",
        "                # patches\n",
        "                # seg = get_patches(seg, kernel_c, kernel_h, kernel_w, step)\n",
        "                seg,shape1 = get_patches_and_bound(seg, kernel_c, kernel_h, kernel_w, step)\n",
        "                if not self.test:\n",
        "                  seg_whole_pad = prepadding(seg_whole,kernel_c,kernel_h,kernel_w,step).unsqueeze(0) \n",
        "                del seg_whole\n",
        "\n",
        "                for index_patches in range(seg.shape[0]):\n",
        "                    # print(img.shape,seg.shape)\n",
        "                    sum_0 = torch.sum(seg[index_patches] == 0)\n",
        "                    sum_numel = seg[index_patches].numel()\n",
        "                    background = sum_0 / sum_numel\n",
        "                    print(sum_0, sum_numel, background)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # if background>0.93 and not TEST:\n",
        "                    if background >= threshold and not TEST:\n",
        "                      if roulette_fix.pop()<roulette:\n",
        "                        pass\n",
        "                      else:\n",
        "                        continue\n",
        "                    #\n",
        "                    ld,lh,lw = find_location(index_patches,shape1,step)\n",
        "                    if not self.test:\n",
        "                      seg_global = crop_and_pad_1_5x(seg_whole_pad,ld,lh,lw,kernel_c, kernel_h, kernel_w).char() \n",
        "\n",
        "\n",
        "                    # uni = torch.unique(seg[index_patches])\n",
        "                    uni = [x for x in range(num_classes)]\n",
        "                    for i in uni:\n",
        "                      sum_item = torch.sum(seg[index_patches] == i)\n",
        "                      self.counts[i] += sum_item.cpu()\n",
        "\n",
        "\n",
        "                    valid_index.append(index_patches)\n",
        "                    sample =  seg[index_patches].unsqueeze(0).cpu()\n",
        "                    #****\n",
        "                    seg_saving_path = os.path.join(working_path,self.datasetname,'seg',str(idx))\n",
        "                    if not os.path.exists(seg_saving_path):\n",
        "                      os.makedirs(seg_saving_path)\n",
        "\n",
        "                    seg_saving_path = os.path.join(seg_saving_path,str(index_patches)+'.npy')\n",
        "                    np.save(seg_saving_path,sample)\n",
        "\n",
        "                    self.segs.append(seg_saving_path)\n",
        "                    #****\n",
        "                    \n",
        "                    if not self.test:\n",
        "                      #seg global\n",
        "                      sample =  seg_global.squeeze(0).cpu()\n",
        "                      del seg_global\n",
        "                      #****\n",
        "                      seg_saving_path = os.path.join(working_path,self.datasetname,'seg_global',str(idx))\n",
        "                      if not os.path.exists(seg_saving_path):\n",
        "                        os.makedirs(seg_saving_path)\n",
        "\n",
        "                      seg_saving_path = os.path.join(seg_saving_path,str(index_patches)+'.npy')\n",
        "                      np.save(seg_saving_path,sample)\n",
        "\n",
        "                      self.segs_global.append(seg_saving_path)\n",
        "                      # #****\n",
        "                    \n",
        "                      # self.seg_names.append(os.path.basename(seg_path))\n",
        "                      import gc\n",
        "                      del sample\n",
        "                    import gc\n",
        "                    gc.collect()\n",
        "                del seg;            \n",
        "                # del seg_whole_pad\n",
        "                import gc\n",
        "                gc.collect()\n",
        "\n",
        "\n",
        "###img\n",
        "                img_path = file_list_img[idx]\n",
        "                img = sitk.ReadImage(img_path, sitk.sitkUInt8)\n",
        "                   \n",
        "                # pre=processing\n",
        "                # img = zero_mean_unit_var(img, msk)\n",
        "            \n",
        "                # ##resample image\n",
        "                # img = resample_image(img, img_spacing, img_size, is_label=False)\n",
        "                # seg = resample_image(seg, img_spacing, img_size, is_label=True)\n",
        "            \n",
        "                # from image  numpy to torch\n",
        "                img = torch.from_numpy(sitk.GetArrayFromImage(img)).unsqueeze(0).to(device)\n",
        "\n",
        "                # # print('shape: ',img.shape,seg.shape)\n",
        "                # # patches\n",
        "                # torch.cuda.empty_cache()\n",
        "                # img = get_patches(img, kernel_c, kernel_h, kernel_w, step)\n",
        "                img_whole = img \n",
        "                # print('shape: ',img.shape,seg.shape)\n",
        "                # patches\n",
        "                # seg = get_patches(seg, kernel_c, kernel_h, kernel_w, step)\n",
        "                img,shape1 = get_patches_and_bound(img, kernel_c, kernel_h, kernel_w, step)\n",
        "                if not self.test:\n",
        "                  img_whole_pad = prepadding(img_whole,kernel_c,kernel_h,kernel_w,step).unsqueeze(0) \n",
        "                del img_whole\n",
        "                gc.collect()\n",
        "                for index_patches in range(img.shape[0]):\n",
        "                    if index_patches  not in valid_index:\n",
        "                        continue\n",
        "                    # print(img.shape,seg.shape)\n",
        "\n",
        "                    ld,lh,lw = find_location(index_patches,shape1,step)\n",
        "                    if not self.test:\n",
        "                      img_global = crop_and_pad_1_5x(img_whole_pad,ld,lh,lw,kernel_c, kernel_h, kernel_w).byte() \n",
        "\n",
        "                    sample = img[index_patches].unsqueeze(0).cpu()\n",
        "                    #****\n",
        "                    img_saving_path = os.path.join(working_path,self.datasetname,'img',str(idx))\n",
        "                    if not os.path.exists(img_saving_path):\n",
        "                      os.makedirs(img_saving_path)\n",
        "                    img_saving_path = os.path.join(img_saving_path,str(index_patches)+'.npy')\n",
        "                    np.save(img_saving_path,sample)\n",
        "             \n",
        "                    self.imgs.append(img_saving_path)\n",
        "                    #*****\n",
        "\n",
        "                    if not self.test:\n",
        "                      sample = img_global.squeeze(0).cpu()\n",
        "                      del img_global\n",
        "                      #****\n",
        "                      img_saving_path = os.path.join(working_path,self.datasetname,'img_global',str(idx))\n",
        "                      if not os.path.exists(img_saving_path):\n",
        "                        os.makedirs(img_saving_path)\n",
        "                      img_saving_path = os.path.join(img_saving_path,str(index_patches)+'.npy')\n",
        "                      np.save(img_saving_path,sample)\n",
        "              \n",
        "                      self.imgs_global.append(img_saving_path)\n",
        "                      #*****\n",
        "                      self.img_names.append(os.path.basename(img_path))\n",
        "                      import gc\n",
        "                      del sample\n",
        "                      gc.collect()\n",
        "                del img;\n",
        "                try:\n",
        "                  del img_whole_pad\n",
        "                except:\n",
        "                  pass\n",
        "                gc.collect()\n",
        "            \n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "\n",
        "        if not self.test:\n",
        "          img_saving_path = self.imgs[item]\n",
        "          seg_saving_path = self.segs[item]\n",
        "          img_saving_path_global = self.imgs_global[item]\n",
        "          seg_saving_path_global = self.segs_global[item]\n",
        "\n",
        "          data_dict = xform({'img': img_saving_path, 'seg': seg_saving_path})\n",
        "          img = data_dict['img'].byte() #uint8   \n",
        "          seg = data_dict['seg'].char() #int8  \n",
        "          data_dict_global = xform_G({'img_global': img_saving_path_global,'seg_global': seg_saving_path_global})\n",
        "          img_global = data_dict_global['img_global'].byte() #uint8   \n",
        "          seg_global = data_dict_global['seg_global']\n",
        "          seg_global = F.interpolate(seg_global.unsqueeze(0), scale_factor=0.5, mode='nearest',recompute_scale_factor=False).squeeze(0).char()\n",
        "          # print('four shapes',img.shape,seg.shape,img_global.shape,seg_global.shape)\n",
        "          return {'img': img, 'seg': seg, 'img_global': img_global,'seg_global': seg_global}\n",
        "      \n",
        "        else:\n",
        "\n",
        "          img_saving_path = self.imgs[item]\n",
        "          img = np.load(img_saving_path)\n",
        "          img=torch.from_numpy(img)\n",
        "\n",
        "          seg_saving_path = self.segs[item]\n",
        "          seg = np.load(seg_saving_path)\n",
        "          seg=torch.from_numpy(seg)\n",
        "\n",
        "          # img_saving_path_global = self.imgs_global[item]\n",
        "          # img_global = np.load(img_saving_path_global)\n",
        "          # img_global=torch.from_numpy(img_global)\n",
        "          img_global = torch.zeros(1)\n",
        "          # seg_saving_path_global = self.segs_global[item]\n",
        "          # seg_global = np.load(seg_saving_path_global)\n",
        "          # seg_global=torch.from_numpy(seg_global)\n",
        "          seg_global = torch.zeros(1)\n",
        "          return {'img': img, 'seg': seg, 'img_global': img_global,'seg_global': seg_global}\n",
        "\n",
        "\n",
        "    def get_sample(self, item):\n",
        "        img_saving_path = self.imgs[item]\n",
        "        img = np.load(img_saving_path)\n",
        "        img=torch.from_numpy(img)\n",
        "\n",
        "        seg_saving_path = self.segs[item]\n",
        "        seg = np.load(seg_saving_path)\n",
        "        seg=torch.from_numpy(seg)\n",
        "\n",
        "        img_saving_path_global = self.imgs_global[item]\n",
        "        img_global = np.load(img_saving_path_global)\n",
        "        img_global=torch.from_numpy(img_global)\n",
        "\n",
        "        seg_saving_path_global = self.segs_global[item]\n",
        "        seg_global = np.load(seg_saving_path_global)\n",
        "        seg_global=torch.from_numpy(seg_global)\n",
        "        return {'img': img, 'seg': seg, 'img_global': img_global,'seg_global': seg_global}\n",
        "    def get_img_name(self, item):\n",
        "        return self.img_names[item]\n",
        "\n",
        "    def get_seg_name(self, item):\n",
        "        return self.seg_names[item]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddjuLDKgrC9f"
      },
      "source": [
        "\n",
        "cuda_dev = '0' #GPU device 0 (can be changed if multiple GPUs are available)\n",
        "\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:\" + cuda_dev if use_cuda else \"cpu\")\n",
        "\n",
        "print('Device: ' + str(device))\n",
        "if use_cuda:\n",
        "    print('GPU: ' + str(torch.cuda.get_device_name(int(cuda_dev))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7fODelBHnIc"
      },
      "source": [
        "### hyperparameter for training and validation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhHr1z9RrFIl"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import argparse\n",
        "rnd_seed = 42 #fixed random seed\n",
        "load_checkpoint = True\n",
        "img_size = [470, 470, 1072]\n",
        "img_spacing = [1,1,1]\n",
        "val_interval = 1\n",
        "num_classes = 3\n",
        "use_amp = True \n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument(\"--load_checkpoint\", type=str,choices=['True','False'],default='False')\n",
        "parser.add_argument(\"--num_epochs\", type=int,default=400)\n",
        "parser.add_argument(\"--learning_rate\", type=float,default=0.0009)\n",
        "parser.add_argument(\"--batch_size\", type=int,default=4)\n",
        "parser.add_argument(\"--criterion_type\", type=str,choices=['CrossEntropy', 'WeightedCrossEntropy', 'WeightedCrossEntropyAdaptive', 'FocalLoss','DiceLoss','W_D'],default='W_D')\n",
        "parser.add_argument(\"--use_amp\", type=str,choices=['True','False'],default='True')\n",
        "parser.add_argument(\"--out_dir\", type=str,default='/content/drive/MyDrive/Computing_Project/publish/meUnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmTjrBvkgc0E"
      },
      "source": [
        "parser.add_argument(\"--kernel_c\", type=int,default=160)\n",
        "parser.add_argument(\"--kernel_h\", type=int,default=160)\n",
        "parser.add_argument(\"--kernel_w\", type=int,default=160)\n",
        "parser.add_argument(\"--step\", type=int,default=80)\n",
        "parser.add_argument(\"--threshold\", type=float,default=0.98)\n",
        "parser.add_argument(\"--roulette\", type=float,default=0.05)\n",
        "parser.add_argument(\"--label_type\", type=str,choices=['all'],default='all')\n",
        "parser.add_argument(\"--data_type\", type=str,choices=['all_data', 'male', 'mated', 'virgin'],default='all_data')\n",
        "parser.add_argument(\"--labelset\", type=str,choices=['original', '1072_470', 'gut'],default='gut')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qg5g00nMBOeo"
      },
      "source": [
        "parser.add_argument(\"--data_dir_root\", type=str,default='/content/drive/MyDrive/Computing_Project/fruit_fly')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXfLBhjO_Q-d"
      },
      "source": [
        "##parse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38OKz42p_QGc"
      },
      "source": [
        "try:\n",
        "#for py\n",
        "  args = parser.parse_args()  \n",
        "  print('This is py')\n",
        "  data_dir_root='/rds/general/user/yw720/home/fruit_fly_google_drive'\n",
        "  current_path = '/rds/general/user/yw720/ephemeral'\n",
        "  import SimpleITK as sitk\n",
        "  MAX_THREADS = 8\n",
        "  sitk.ProcessObject.SetGlobalDefaultNumberOfThreads(MAX_THREADS) \n",
        "except:\n",
        "#for jupyter\n",
        "  print('This is jupyter')\n",
        "  args = parser.parse_args(args=[])\n",
        "  # data_dir_combination_root = args.data_dir_combination_root\n",
        "  data_dir_root = args.data_dir_root\n",
        "print(args)\n",
        "if args.load_checkpoint == 'True':\n",
        "    load_checkpoint = True\n",
        "num_epochs = args.num_epochs\n",
        "learning_rate = args.learning_rate\n",
        "batch_size = args.batch_size\n",
        "criterion_type =args.criterion_type\n",
        "if args.use_amp == 'False':\n",
        "  use_amp = False\n",
        "out_dir = args.out_dir\n",
        "if not os.path.exists(out_dir):\n",
        "    os.makedirs(out_dir)\n",
        "figure_dir = out_dir+'/Figure/'\n",
        "figure_dir = os.path.join(out_dir, 'Figure_separate2')\n",
        "if not os.path.exists(figure_dir):\n",
        "    os.makedirs(figure_dir)\n",
        "kernel_c = args.kernel_c; kernel_h=args.kernel_h; kernel_w = args.kernel_w\n",
        "step = args.step;\n",
        "threshold = args.threshold\n",
        "roulette = args.roulette\n",
        "label_type = args.label_type\n",
        "data_type = args.data_type\n",
        "labelset = args.labelset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlO6xcc2A1PR"
      },
      "source": [
        "working_path = os.path.join(current_path, 'save'+out_dir[-8:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgDkrAgBrMd5"
      },
      "source": [
        "#Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbh7futKDK4O"
      },
      "source": [
        "##load dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rh-mAkatXWxL"
      },
      "source": [
        "Load male data dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1w8vt9WdwR6"
      },
      "source": [
        "if labelset == 'original':\n",
        "  data_dir_combination = data_dir_root+'/male_seg/' \n",
        "if labelset == '1072_470':\n",
        "  data_dir_combination =  data_dir_root+'/male_seg_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir_combination =data_dir_root+ '/male_seg_gut/'\n",
        "# data_dir_combination = data_dir_combination_root+'male_seg{}/'.format(labelset)\n",
        "#segmentation\n",
        "\n",
        "list_male_seg = [data_dir_combination + 'M1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_male_seg +=[data_dir_combination + 'M2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "#data\n",
        "if labelset == 'original':\n",
        "  data_dir = data_dir_root+'/male_microCT/'\n",
        "if labelset == '1072_470':\n",
        "  data_dir = data_dir_root+'/male_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir = data_dir_root+'/male_gut/'\n",
        "# data_dir = data_dir_root++'male{}/'.format(labelset)\n",
        "list_male = [data_dir + 'M1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_male +=[data_dir + 'M2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vXnQYxnXZVL"
      },
      "source": [
        "load mated data dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szdPsg-1XkEl"
      },
      "source": [
        "if labelset == 'original':\n",
        "  data_dir_combination = data_dir_root+'/mated_seg/'\n",
        "if labelset == '1072_470':\n",
        "  data_dir_combination = data_dir_root+'/mated_seg_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir_combination = data_dir_root+'/mated_seg_gut/'\n",
        "#segmentation\n",
        "\n",
        "list_mated_seg =  [data_dir_combination + 'F1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_mated_seg +=[data_dir_combination + 'F2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "\n",
        "#data\n",
        "if labelset == 'original':\n",
        "  data_dir = data_dir_root+'/mated_microCT/'\n",
        "if labelset == '1072_470':\n",
        "  data_dir = data_dir_root+'/mated_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir = data_dir_root+'/mated_gut/'\n",
        "list_mated = [data_dir + 'F1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_mated +=[data_dir + 'F2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQ_o21x3YRPr"
      },
      "source": [
        "load virgin data dir"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1orJydJrYTa_"
      },
      "source": [
        "if labelset == 'original':\n",
        "  data_dir_combination = data_dir_root+'/virgin_seg/'\n",
        "if labelset == '1072_470':\n",
        "  data_dir_combination = data_dir_root+'/virgin_seg_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir_combination = data_dir_root+'/virgin_seg_gut/'\n",
        "#segmentation\n",
        "\n",
        "list_virgin_seg = [data_dir_combination + 'V1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_virgin_seg +=[data_dir_combination + 'V2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "\n",
        "# #data\n",
        "if labelset == 'original':\n",
        "  data_dir = data_dir_root+'/virgin_microCT/'\n",
        "if labelset == '1072_470':\n",
        "  data_dir = data_dir_root+'/virgin_1072_470/'\n",
        "if labelset == 'gut':\n",
        "  data_dir = data_dir_root+'/virgin_gut/'\n",
        "list_virgin = [data_dir + 'V1{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n",
        "list_virgin +=[data_dir + 'V2{}.nii.gz'.format(chr(ord('A')+x)) for x in range(0,5)]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc79S6m9n3nR"
      },
      "source": [
        "load train and validation data file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4xCLzDyrHOD"
      },
      "source": [
        "if data_type == 'all_data':\n",
        "    #img\n",
        "    files_seg_img_train = list_male[0:5]+ list_male[7:9] + list_mated[0:5] + list_mated[7:9] + list_virgin[0:5] + list_virgin[7:9]\n",
        "    #seg\n",
        "    files_seg_seg_train = list_male_seg[0:5]+ list_male_seg[7:9] + list_mated_seg[0:5] + list_mated_seg[7:9] + list_virgin_seg[0:5] + list_virgin_seg[7:9]\n",
        "    #img\n",
        "    files_seg_img_val = list_male[9:10] + list_mated[9:10] + list_virgin[9:10]\n",
        "    #seg\n",
        "    files_seg_seg_val = list_male_seg[9:10] + list_mated_seg[9:10] + list_virgin_seg[9:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD6YjB7ef_Cj"
      },
      "source": [
        "##load train and val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Go2--S4rmAU"
      },
      "source": [
        "# LOAD TRAINING DATA separate\n",
        "dataset_train = ImageSegmentationDataset_separate(files_seg_img_train, files_seg_seg_train, img_spacing, img_size,True,kernel_c, kernel_h, kernel_w,step,False)\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers = 4, prefetch_factor=2,pin_memory = True)\n",
        "\n",
        "# LOAD VALIDATION DATA\n",
        "#let  step = kernel_c for validation set\n",
        "dataset_val = ImageSegmentationDataset_separate(files_seg_img_val, files_seg_seg_val, img_spacing,img_size,True,int(kernel_c*1.5), int(kernel_h*1.5), int(kernel_w*1.5),int(kernel_c*1.5)  ,True)\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size, shuffle=False, num_workers = 4, prefetch_factor=2,pin_memory = True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sFHpSNbUrrUC"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhX5qLUHnlM2"
      },
      "source": [
        "##model: meU-net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Y5x60Km444"
      },
      "source": [
        "import torch.utils.checkpoint as cp\n",
        "class DummyLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dummy = nn.Parameter(torch.ones(1, dtype=torch.float32))\n",
        "    def forward(self,x):\n",
        "        return x + self.dummy - self.dummy #(also tried x+self.dummy)\n",
        "\n",
        "def conv_block(in_chan, out_chan, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv3d(in_chan, out_chan, kernel_size=3, padding=1, stride=stride),\n",
        "        nn.BatchNorm3d(out_chan),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "def conv_stage(in_chan, out_chan):\n",
        "    return nn.Sequential(\n",
        "        conv_block(in_chan, out_chan),\n",
        "        conv_block(out_chan, out_chan),\n",
        "    )\n",
        "\n",
        "\n",
        "class UNet3d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dummy_layer = DummyLayer()\n",
        "        self.enc1 = conv_stage(1, 16)\n",
        "        self.enc2 = conv_stage(16, 32)\n",
        "        self.enc3 = conv_stage(32, 64)\n",
        "        self.enc4 = conv_stage(64, 128)\n",
        "        self.enc5 = conv_stage(128, 128)\n",
        "        self.pool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "        self.dec4 = conv_stage(256, 64)\n",
        "        self.dec3 = conv_stage(128, 32)\n",
        "        self.dec2 = conv_stage(64, 16)\n",
        "        self.dec1 = conv_stage(32, 16)\n",
        "        self.conv_out = nn.Conv3d(16, 3, 1)\n",
        "        self.global_out = nn.Conv3d(16, 3, 1)\n",
        "\n",
        "    def forward(self, x, x_global):\n",
        "      #x: standard patch x_global: expanded patch\n",
        "      \n",
        "      if self.training:\n",
        "        #training mode\n",
        "\n",
        "        with torch.no_grad():\n",
        "          enc1_global = cp.checkpoint(self.enc1,x_global)\n",
        "          pool1 = self.pool(enc1_global)\n",
        "        \n",
        "        #dummy_layer required by checkpointing \n",
        "        pool1 = self.dummy_layer(pool1) \n",
        "        enc2_global = cp.checkpoint(self.enc2,pool1)\n",
        "        enc3_global = cp.checkpoint(self.enc3,self.pool(enc2_global))\n",
        "        enc4_global = cp.checkpoint(self.enc4,self.pool(enc3_global))\n",
        "        enc5_global = cp.checkpoint(self.enc5,self.pool(enc4_global))          \n",
        "\n",
        "        dec4_global = cp.checkpoint(self.dec4,torch.cat((enc4_global, F.interpolate(enc5_global, enc4_global.size()[2:], mode='nearest')), 1))\n",
        "        dec3_global = cp.checkpoint(self.dec3,torch.cat((enc3_global, F.interpolate(dec4_global, enc3_global.size()[2:], mode='nearest')), 1))\n",
        "        dec2_global = cp.checkpoint(self.dec2,torch.cat((enc2_global, F.interpolate(dec3_global, enc2_global.size()[2:], mode='nearest')), 1))\n",
        "        dec2_global = self.global_out(dec2_global)\n",
        "        \n",
        "        \n",
        "        x = self.dummy_layer(x)\n",
        "        enc1 = cp.checkpoint(self.enc1, x)\n",
        "        enc2 = cp.checkpoint(self.enc2,self.pool(enc1))\n",
        "        enc3 = cp.checkpoint(self.enc3,self.pool(enc2))\n",
        "        enc4 = cp.checkpoint(self.enc4,self.pool(enc3))\n",
        "        enc5 = cp.checkpoint(self.enc5,self.pool(enc4))\n",
        "\n",
        "        dec4 = cp.checkpoint(self.dec4,torch.cat((enc4, F.interpolate(enc5, enc4.size()[2:], mode='nearest')), 1))\n",
        "        dec3 = cp.checkpoint(self.dec3,torch.cat((enc3, F.interpolate(dec4, enc3.size()[2:], mode='nearest')), 1))\n",
        "        dec2 = cp.checkpoint(self.dec2,torch.cat((enc2, F.interpolate(dec3, enc2.size()[2:], mode='nearest')), 1))\n",
        "        dec1 = cp.checkpoint(self.dec1,torch.cat((enc1, F.interpolate(dec2, enc1.size()[2:], mode='nearest')), 1))    \n",
        "\n",
        "        out = self.conv_out(dec1)\n",
        "        return (out,dec2_global)\n",
        "      else:\n",
        "        #validation mode\n",
        "\n",
        "        dec2_global = None \n",
        "        x = self.dummy_layer(x)\n",
        "        enc1 = cp.checkpoint(self.enc1, x)\n",
        "        enc2 = cp.checkpoint(self.enc2,self.pool(enc1))\n",
        "        enc3 = cp.checkpoint(self.enc3,self.pool(enc2))\n",
        "        enc4 = cp.checkpoint(self.enc4,self.pool(enc3))\n",
        "        enc5 = cp.checkpoint(self.enc5,self.pool(enc4))\n",
        "\n",
        "        dec4 = cp.checkpoint(self.dec4,torch.cat((enc4, F.interpolate(enc5, enc4.size()[2:], mode='nearest')), 1))\n",
        "        dec3 = cp.checkpoint(self.dec3,torch.cat((enc3, F.interpolate(dec4, enc3.size()[2:], mode='nearest')), 1))\n",
        "        dec2 = cp.checkpoint(self.dec2,torch.cat((enc2, F.interpolate(dec3, enc2.size()[2:], mode='nearest')), 1))\n",
        "        dec1 = cp.checkpoint(self.dec1,torch.cat((enc1, F.interpolate(dec2, enc1.size()[2:], mode='nearest')), 1))    \n",
        "\n",
        "        out = self.conv_out(dec1)\n",
        "        return (out,dec2_global)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeOsos2Gy_Is"
      },
      "source": [
        "##criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDpZmbhBTJ2e"
      },
      "source": [
        "###weight"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rCI6Rpuffiy"
      },
      "source": [
        "import numpy as np\n",
        "def soft_max(weight):\n",
        "    weight = weight/np.sum(weight)\n",
        "    print(weight)\n",
        "    expo = np.exp(1*weight)\n",
        "    sum_expo = np.sum(expo)\n",
        "    return expo/sum_expo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkNKgWRlTKyH"
      },
      "source": [
        "# count the number in patches for weight instead of in total data\n",
        "seg_path = files_seg_seg_train[0]\n",
        "seg = sitk.ReadImage(seg_path,sitk.sitkInt8)\n",
        "seg = torch.from_numpy(sitk.GetArrayFromImage(seg))\n",
        "count_list=[]\n",
        "uni = torch.unique(seg)\n",
        "counts = dataset_train.counts\n",
        "for i in uni:\n",
        "    n = counts[i]\n",
        "    print('number of {}: '.format(i),n)\n",
        "    count_list.append((n.numpy()))\n",
        "\n",
        "weight = 1/(count_list/sum(count_list))\n",
        "weight = soft_max(weight)\n",
        "weight = weight.tolist()\n",
        "weight_= []\n",
        "weight_max = max(weight)\n",
        "for i in range(num_classes):\n",
        "  if i in uni:\n",
        "    weight_.append(weight.pop(0))\n",
        "  else:\n",
        "    weight_.append(0)\n",
        "del seg\n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20E3QTEVh8FE"
      },
      "source": [
        "weight_ = torch.FloatTensor(weight_).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cU4dyLVKzedV"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "CrossEntropy = nn.CrossEntropyLoss()\n",
        "WeightedCrossEntropy = nn.CrossEntropyLoss(weight = weight_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDCPSYmHx06f"
      },
      "source": [
        "focal loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjy3-m65x06f"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FocalLoss_class(nn.modules.loss._WeightedLoss):\n",
        "    def __init__(self, weight=None, gamma=2,reduction='mean'):\n",
        "        super(FocalLoss_class, self).__init__(weight,reduction=reduction)\n",
        "        self.gamma = gamma\n",
        "        self.weight = weight #weight parameter will act as the alpha parameter to balance class weights\n",
        "\n",
        "    def forward(self, input, target):\n",
        "\n",
        "        ce_loss = F.cross_entropy(input, target,reduction=self.reduction,weight=self.weight)\n",
        "        pt = torch.exp(-ce_loss)\n",
        "        focal_loss = (torch.pow((1 - pt), self.gamma) * ce_loss).mean()\n",
        "        return focal_loss\n",
        "# from https://github.com/gokulprasadthekkel/pytorch-multi-class-focal-loss/blob/master/focal_loss.py\n",
        "# FocalLoss = FocalLoss_class(weight = weight_)\n",
        "FocalLoss = FocalLoss_class(weight = None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZLK4tR9yWsQ"
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "def make_one_hot(vol, mask):\n",
        "\n",
        "    lens = len(mask)\n",
        "    shape = np.array(vol.shape)\n",
        "    shape[1] = lens\n",
        "    shape = tuple(shape)\n",
        "    result = torch.zeros(shape,dtype = vol.dtype).to(vol.device)\n",
        "\n",
        "    for idx, label in enumerate(mask):\n",
        "        tmp = vol == label\n",
        "        result[:,idx] = tmp.squeeze(1)\n",
        "\n",
        "    return result\n",
        "\n",
        "class dice_loss(nn.Module):\n",
        "    '''\n",
        "    vol1,vol2: need to make one hot first\n",
        "    '''\n",
        "    def __init__(self, epsilon=1e-5):\n",
        "        super(dice_loss, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def forward(self, vol1, vol2):\n",
        "        shape = vol1.shape\n",
        "        total_loss = 0\n",
        "        uni = [x for x in range(num_classes)]\n",
        "        vol2 = make_one_hot(vol2,uni)\n",
        "        vol2.requires_grad = False\n",
        "        import time\n",
        "\n",
        "        for i in range(shape[1]):\n",
        "\n",
        "            top = 2 * torch.sum(torch.mul(vol1[:,i],vol2[:,i] ))\n",
        "\n",
        "            bottom = torch.sum(vol1[:,i]) + torch.sum(vol2[:,i])\n",
        "            bottom +=self.epsilon\n",
        "            loss_tmp = 1 -  (top / bottom)\n",
        "            total_loss += loss_tmp\n",
        "            import time\n",
        "\n",
        "\n",
        "        return total_loss / shape[1]\n",
        "DiceLoss = dice_loss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFz41Qiz8Vv"
      },
      "source": [
        "###selection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2KfKJMc8Pds"
      },
      "source": [
        "if criterion_type not in ['CrossEntropy', 'WeightedCrossEntropy' , 'FocalLoss','DiceLoss','W_D']:\n",
        "  print('please input valid criterion_type')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DzoDMPYgz5nU"
      },
      "source": [
        "def criterion(prediction,target,weight):\n",
        "\n",
        "  if criterion_type == 'CrossEntropy':\n",
        "    return CrossEntropy(prediction,target)\n",
        "  elif criterion_type == 'WeightedCrossEntropy':\n",
        "    return WeightedCrossEntropy(prediction,target)\n",
        "  elif criterion_type == 'WeightedCrossEntropyAdaptive':\n",
        "    prediction = F.softmax(prediction,dim = 1)\n",
        "    weight_list = []\n",
        "    N = target.numel()\n",
        "    uni = [x for x in range(num_classes)]\n",
        "    target1 = make_one_hot(target.unsqueeze(1),uni).detach()\n",
        "    res = (target1 * prediction).detach()\n",
        "    \n",
        "    for i in range(num_classes):\n",
        "      sum_pn = torch.sum(res[:,i])\n",
        "      weight_list.append((N-sum_pn)/sum_pn)\n",
        "    weight = torch.FloatTensor(weight_list).to(device).detach()\n",
        "    return F.cross_entropy(prediction, target, weight = weight)\n",
        "\n",
        "  elif criterion_type == 'FocalLoss':\n",
        "    return FocalLoss(prediction,target)\n",
        "  elif criterion_type == 'DiceLoss':\n",
        "    return DiceLoss(F.softmax(prediction,dim = 1),target.unsqueeze(1).byte())\n",
        "  elif criterion_type == 'W_D':\n",
        "    return DiceLoss(F.softmax(prediction,dim = 1),target.unsqueeze(1).byte()) + WeightedCrossEntropy(prediction,target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jq388Sg_xyuY"
      },
      "source": [
        "def save_checkpoint(epoch,model,optimizer,scaler,best_val,path):\n",
        "  torch.save({\n",
        "              'epoch': epoch,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict': optimizer.state_dict(),\n",
        "              'scaler': scaler.state_dict(),\n",
        "              'best_val': best_val,\n",
        "              }, path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyuV7-Atv4eQ"
      },
      "source": [
        "## training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QGlzJdHgr97I"
      },
      "source": [
        "from sklearn import metrics\n",
        "import sklearn.metrics\n",
        "model_dir = os.path.join(out_dir, 'model')\n",
        "if not os.path.exists(model_dir):\n",
        "    os.makedirs(model_dir)\n",
        "\n",
        "torch.manual_seed(rnd_seed) #fix random seed\n",
        "\n",
        "loss_train_log = []\n",
        "loss_val_log = []\n",
        "epoch_val_log = []\n",
        "best_val = 9999\n",
        "seg_val_idx = 1018\n",
        "seg_val = None\n",
        "start_epoch = 0\n",
        "early_stopping_threshold = 30\n",
        "early_stopping_count = 0\n",
        "\n",
        "\n",
        "if load_checkpoint == False:\n",
        "  model = UNet3d().to(device)\n",
        "  model.train()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "else:\n",
        "  print('loading the checkpoint')\n",
        "  model = UNet3d().to(device)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "  PATH = save_path = os.path.join(model_dir, 'model.pt')\n",
        "  checkpoint = torch.load(PATH)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  scaler.load_state_dict(checkpoint['scaler'])\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  best_val = checkpoint['best_val']\n",
        "  model.train()\n",
        "  print('start_epoch = ',start_epoch)\n",
        "  torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "\n",
        "print('START TRAINING...')\n",
        "import time\n",
        "\n",
        "time_start=time.time()\n",
        "for epoch in range(1+start_epoch, num_epochs + 1+start_epoch):\n",
        "\n",
        "    # Training\n",
        "    model.train()\n",
        "    for batch_idx, batch_samples in enumerate(dataloader_train):\n",
        "        img, seg = batch_samples['img'].to(device).float(), batch_samples['seg'].to(device)\n",
        "        img_global = batch_samples['img_global'].to(device).float()\n",
        "        # print(batch_idx)\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "          prd = model(img,img_global)\n",
        "          del img;del img_global;gc.collect()\n",
        "          seg_global = batch_samples['seg_global'].to(device)\n",
        "          loss = criterion(prd[0], seg.squeeze(1).long(),weight = weight_) +  criterion(prd[1], seg_global.squeeze(1).long(),weight = weight_)\n",
        "          del seg; del seg_global\n",
        "          del prd;gc.collect()\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        lossitem = loss.item()\n",
        "        del loss; gc.collect()\n",
        "        # optimizer.step()\n",
        "        if load_checkpoint and epoch == 1+start_epoch:\n",
        "          optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "          start_epoch = 0\n",
        "          print('reload optimizer')\n",
        "          del checkpoint\n",
        "          gc.collect()\n",
        "          torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    loss_train_log.append(lossitem)\n",
        "        \n",
        "    print('+ TRAINING \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, lossitem))\n",
        "\n",
        "\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_interval = 1\n",
        "    if epoch == 1 or epoch == 2 or (epoch % val_interval == 0 and epoch>20):\n",
        "        loss_val = 0\n",
        "        sum_pts = 0\n",
        "        torch.cuda.empty_cache() \n",
        "        with torch.no_grad():\n",
        "          with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "            # criterion_val = FocalLoss(weight = criterion.weight,gamma =criterion.gamma,reduction= 'sum' )\n",
        "            for idx,data_sample in enumerate(dataloader_val):\n",
        "                img, seg = data_sample['img'].to(device).half(), data_sample['seg'].to(device)\n",
        "                # img_global = data_sample['img_global'].to(device).half()\n",
        "                # prd = model(img)\n",
        "                img_global = None\n",
        "                prd = model(img,img_global)\n",
        "\n",
        "                loss_val +=  DiceLoss(F.softmax(prd[0],dim = 1), seg).item()\n",
        "                # sum_pts += np.prod(img_size)\n",
        "                sum_pts += 1\n",
        "        loss_val /= sum_pts\n",
        "\n",
        "        loss_val_log.append(loss_val)\n",
        "        epoch_val_log.append(epoch)\n",
        "\n",
        "        if loss_val < best_val:\n",
        "          best_val = loss_val\n",
        "          save_path = os.path.join(model_dir, 'model.pt')\n",
        "          save_checkpoint(epoch,model,optimizer,scaler,best_val,save_path)\n",
        "          early_stopping_count = 0\n",
        "          \n",
        "        else:\n",
        "          early_stopping_count += val_interval\n",
        "        \n",
        "        if early_stopping_count>early_stopping_threshold:\n",
        "          print('no improvement, triggering early stopping')\n",
        "          break\n",
        "\n",
        "\n",
        "        print('--------------------------------------------------')\n",
        "        print('+ VALIDATE \\tEpoch: {} \\tLoss: {:.6f}'.format(epoch, loss_val))\n",
        "        print('--------------------------------------------------')\n",
        "        del img;del img_global\n",
        "        del seg;\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "time_end=time.time()\n",
        "\n",
        "print('\\nFinished TRAINING.')\n",
        "print('Time cost for training: ',time_end-time_start,'s')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDoZi2LtrKQL"
      },
      "source": [
        "save_path = os.path.join(model_dir, 'model_last.pt')\n",
        "save_checkpoint(epoch,model,optimizer,scaler,best_val,save_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLFlb3y-UMGh"
      },
      "source": [
        "clean RAM for further testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTYDWgiNUN8Y"
      },
      "source": [
        "try:del prd \n",
        "except:pass\n",
        "try:del img \n",
        "except:pass\n",
        "try:del seg \n",
        "except:pass\n",
        "try:del loss \n",
        "except:pass\n",
        "try:del img_global\n",
        "except:pass\n",
        "del optimizer\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "del dataset_train \n",
        "del dataloader_train\n",
        "del dataset_val \n",
        "del dataloader_val \n",
        "gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEakJbA1kjIP"
      },
      "source": [
        "#Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVf-tbQkkjIQ"
      },
      "source": [
        "##model test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huzCdWkDkjIQ"
      },
      "source": [
        "import torch.utils.checkpoint as cp\n",
        "class DummyLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dummy = nn.Parameter(torch.ones(1, dtype=torch.float32))\n",
        "    def forward(self,x):\n",
        "        return x + self.dummy - self.dummy #(also tried x+self.dummy)\n",
        "\n",
        "def conv_block(in_chan, out_chan, stride=1):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv3d(in_chan, out_chan, kernel_size=3, padding=1, stride=stride),\n",
        "        nn.BatchNorm3d(out_chan),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def conv_stage(in_chan, out_chan):\n",
        "    return nn.Sequential(\n",
        "        conv_block(in_chan, out_chan),\n",
        "        conv_block(out_chan, out_chan),\n",
        "    )\n",
        "\n",
        "\n",
        "class UNet3d(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dummy_layer = DummyLayer()\n",
        "        self.enc1 = conv_stage(1, 16)\n",
        "        self.enc2 = conv_stage(16, 32)\n",
        "        self.enc3 = conv_stage(32, 64)\n",
        "        self.enc4 = conv_stage(64, 128)\n",
        "        self.enc5 = conv_stage(128, 128)\n",
        "        self.pool = nn.MaxPool3d(2, 2)\n",
        "\n",
        "        self.dec4 = conv_stage(256, 64)\n",
        "        self.dec3 = conv_stage(128, 32)\n",
        "        self.dec2 = conv_stage(64, 16)\n",
        "        self.dec1 = conv_stage(32, 16)\n",
        "        self.conv_out = nn.Conv3d(16, 3, 1)\n",
        "\n",
        "        self.global_out = nn.Conv3d(16, 3, 1)\n",
        "\n",
        "    def forward(self, x, x_global):\n",
        "       \n",
        "\n",
        "        dec2_global = None\n",
        "        \n",
        "        x = self.dummy_layer(x)\n",
        "        enc1 = cp.checkpoint(self.enc1, x)\n",
        "        enc2 = cp.checkpoint(self.enc2,self.pool(enc1))\n",
        "        enc3 = cp.checkpoint(self.enc3,self.pool(enc2))\n",
        "        enc4 = cp.checkpoint(self.enc4,self.pool(enc3))\n",
        "        enc5 = cp.checkpoint(self.enc5,self.pool(enc4))\n",
        "\n",
        "        dec4 = cp.checkpoint(self.dec4,torch.cat((enc4, F.interpolate(enc5, enc4.size()[2:], mode='nearest')), 1))\n",
        "        dec3 = cp.checkpoint(self.dec3,torch.cat((enc3, F.interpolate(dec4, enc3.size()[2:], mode='nearest')), 1))\n",
        "        dec2 = cp.checkpoint(self.dec2,torch.cat((enc2, F.interpolate(dec3, enc2.size()[2:], mode='nearest')), 1))\n",
        "        dec1 = cp.checkpoint(self.dec1,torch.cat((enc1, F.interpolate(dec2, enc1.size()[2:], mode='nearest')), 1))    \n",
        "\n",
        "        out = self.conv_out(dec1)\n",
        "        return (out,dec2_global)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6Z01mS-kjIQ"
      },
      "source": [
        "## hyperparameter for testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTG5mSs-kjIQ"
      },
      "source": [
        "img_size_original = [470,470,1072]\n",
        "kernel_c = 240; kernel_h=240; kernel_w = 240\n",
        "step = 120;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcTW4JFUkjIQ"
      },
      "source": [
        "H,W,C = img_size\n",
        "def cal_shape_padding(n,k,stride,p=0):\n",
        "    #the two boundry: left  right\n",
        "    total_padding = stride - (n - (k+(int((n-k+2*p)/stride))*stride))\n",
        "    if total_padding == stride:\n",
        "      total_padding = 0        \n",
        "    return  n + total_padding\n",
        "\n",
        "C_ = cal_shape_padding(C,kernel_c,step)\n",
        "H_ = cal_shape_padding(H,kernel_h,step)\n",
        "W_ = cal_shape_padding(W,kernel_w,step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFAJgsqNkjIQ"
      },
      "source": [
        "def cal_shape(batch_size, C, H, W, kernel_c, kernel_h, kernel_w,\n",
        "                    step):\n",
        "    def cal(n, k, s):\n",
        "        return int((n - k) / s) + 1\n",
        "\n",
        "    i0 = cal(C, kernel_c, step)\n",
        "    i1 = cal(H, kernel_h, step)\n",
        "    i2 = cal(W, kernel_w, step)\n",
        "    i3 = batch_size\n",
        "    i4 = kernel_c\n",
        "    i5 = kernel_h\n",
        "    i6 = kernel_w\n",
        "    return i0*i1*i2*i3\n",
        "num_patch_per_block = cal_shape(1, C_, H_, W_, kernel_c, kernel_h, kernel_w,\n",
        "                    step)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6fTdSiDkjIR"
      },
      "source": [
        "##load test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4awKDB5kjIR"
      },
      "source": [
        "if data_type == 'all_data':\n",
        "  if label_type == 'all':\n",
        "    #img\n",
        "    files_seg_img_test = list_male[5:7] +list_mated[5:7] + list_virgin[5:7]\n",
        "    #seg\n",
        "    files_seg_seg_test = list_male_seg[5:7]+list_mated_seg[5:7] + list_virgin_seg[5:7]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGvG7_A4kjIR"
      },
      "source": [
        "#Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9ODqmbYkjIR"
      },
      "source": [
        "##utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJDpwZUDkjIR"
      },
      "source": [
        "def display_image(img, x=None, y=None, z=None, window=None, level=None, colormap='gray', crosshair=False,figure_name=''):\n",
        "    # Convert SimpleITK image to NumPy array\n",
        "    img_array = sitk.GetArrayFromImage(img)\n",
        "\n",
        "    # Get image dimensions in millimetres\n",
        "    size = img.GetSize()\n",
        "    spacing = img.GetSpacing()\n",
        "    width  = size[0] * spacing[0]\n",
        "    height = size[1] * spacing[1]\n",
        "    depth  = size[2] * spacing[2]\n",
        "\n",
        "    if x is None:\n",
        "        x = np.floor(size[0]/2).astype(int)\n",
        "    if y is None:\n",
        "        y = np.floor(size[1]/2).astype(int)\n",
        "    if z is None:\n",
        "        z = np.floor(size[2]/2).astype(int)\n",
        "\n",
        "    if window is None:\n",
        "        window = np.max(img_array) - np.min(img_array)\n",
        "\n",
        "    if level is None:\n",
        "        level = window / 2 + np.min(img_array)\n",
        "\n",
        "    low,high = wl_to_lh(window,level)\n",
        "\n",
        "    # Display the orthogonal slices\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(10, 4))\n",
        "\n",
        "    ax1.imshow(img_array[z,:,:], cmap=colormap, clim=(low, high), extent=(0, width, height, 0))\n",
        "    ax2.imshow(img_array[:,y,:], origin='lower', cmap=colormap, clim=(low, high), extent=(0, width,  0, depth))\n",
        "    ax3.imshow(img_array[:,:,x], origin='lower', cmap=colormap, clim=(low, high), extent=(0, height, 0, depth))\n",
        "    ax1.set_xticks([])\n",
        "    ax1.set_yticks([])\n",
        "    ax2.set_xticks([])\n",
        "    ax2.set_yticks([])  \n",
        "    ax3.set_xticks([])\n",
        "    ax3.set_yticks([]) \n",
        "    # Additionally display crosshairs\n",
        "    if crosshair:\n",
        "        ax1.axhline(y * spacing[1], lw=1)\n",
        "        ax1.axvline(x * spacing[0], lw=1)\n",
        "        ax2.axhline(z * spacing[2], lw=1)\n",
        "        ax2.axvline(x * spacing[0], lw=1)\n",
        "        ax3.axhline(z * spacing[2], lw=1)\n",
        "        ax3.axvline(y * spacing[1], lw=1)\n",
        "    if figure_name != '':\n",
        "      # plt.savefig(os.path.join(figure_dir,str(figure_name)+'.jpg'))\n",
        "      extent = ax1.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "      fig.savefig(os.path.join(figure_dir,str(figure_name)+'_1.jpg'), bbox_inches=extent)\n",
        "      extent = ax2.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "      fig.savefig(os.path.join(figure_dir,str(figure_name)+'_2.jpg'), bbox_inches=extent)\n",
        "      extent = ax3.get_window_extent().transformed(fig.dpi_scale_trans.inverted())\n",
        "      fig.savefig(os.path.join(figure_dir,str(figure_name)+'_3.jpg'), bbox_inches=extent)\n",
        "    plt.show()\n",
        "\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def load_original_image_torch(idx):\n",
        "\n",
        "  # img_path = file_list_img[idx]\n",
        "  # seg_path = file_list_seg[idx]\n",
        "  print(idx)\n",
        "  img_path = files_seg_img_test[idx]\n",
        "  # msk_path = file_list_msk[idx]\n",
        "  # img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
        "  img_original = sitk.ReadImage(img_path,sitk.sitkUInt8)\n",
        "  # seg = sitk.GetArrayFromImage(seg)\n",
        "  img = torch.from_numpy(sitk.GetArrayFromImage(img_original))\n",
        "  # seg = prepadding(seg.unsqueeze(0),kernel_c,kernel_h,kernel_w,step)\n",
        "\n",
        "  \n",
        "  #display\n",
        "  img_display = sitk.GetImageFromArray(img.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "  # img_display = sitk.LabelToRGB(img_display)\n",
        "  display_image(img_display,figure_name = 'original_image'+str(idx))\n",
        "  del img_display\n",
        "\n",
        "  return img_original\n",
        "\n",
        "def weight_patch(batch_size,num_classes,C,H,W):\n",
        "    import torch\n",
        "\n",
        "    weight_mask = torch.ones((batch_size,num_classes,C,H,W),dtype = torch.int8)\n",
        "    len_c = C//2\n",
        "    len_h = H//2\n",
        "    len_w = W//2\n",
        "    front,back = int(len_c/2),len_c + int(len_c/2)\n",
        "    top,bottom = int(len_h/2),len_h + int(len_h/2)\n",
        "    left,right = int(len_w/2),len_w + int(len_w/2)\n",
        "\n",
        "    weight_mask[:,:,front:back,top:bottom,left:right] = 2\n",
        "    return weight_mask\n",
        "\n",
        "def patch_combine(windows,batch_size=1, C=96, H=96, W=96, kernel_c = 32, kernel_h=32, kernel_w = 32,step = 32 ):\n",
        "\n",
        "    import torch\n",
        "\n",
        "    def calculate_shape(batch_size, C, H, W, kernel_c, kernel_h, kernel_w,\n",
        "                        step):\n",
        "        def cal(n, k, s):\n",
        "            return int((n - k) / s) + 1\n",
        "\n",
        "        i0 = cal(C, kernel_c, step)\n",
        "        i1 = cal(H, kernel_h, step)\n",
        "        i2 = cal(W, kernel_w, step)\n",
        "        i3 = batch_size\n",
        "        i4 = kernel_c\n",
        "        i5 = kernel_h\n",
        "        i6 = kernel_w\n",
        "        return (i0, i1, i2, i3, i4, i5, i6)\n",
        "\n",
        "    shapes = calculate_shape(batch_size, C, H, W,kernel_c,kernel_h,kernel_w,step)\n",
        "\n",
        "    windows = windows.reshape(shapes)\n",
        "\n",
        "\n",
        "    windows  = windows.permute(3, 0, 1, 2, 4, 5, 6)\n",
        "\n",
        "    shape1 = windows.shape\n",
        "\n",
        "    print(C,H,W)\n",
        "\n",
        "    repatch_com = torch.zeros((batch_size,num_classes,C,H,W),dtype = torch.int8).to(windows.device)\n",
        "    print(repatch_com.shape)\n",
        "\n",
        "\n",
        "    kernel_c, kernel_h, kernel_w\n",
        "    for b in range(batch_size):\n",
        "        for c in range(shape1[1]):\n",
        "            for h in range(shape1[2]):\n",
        "                for w in range(shape1[3]):\n",
        "                    # print(c,h,w)\n",
        "                    # repatch_com[:,c*kernel_c:(c+1)*kernel_c,h*kernel_h:(h+1)*kernel_h,w*kernel_w:(w+1)*kernel_w] = repatch_reshape[:,c,h,w,:,:,:]\n",
        "                    repatch_com[b,:,c*step:c*step+kernel_c,h*step:h*step+kernel_h,w*step:w*step+kernel_w] += make_one_hot(windows[b,c,h,w,:,:,:].unsqueeze(0).unsqueeze(0),[x for x in range(num_classes)]).squeeze() *weight_patch(batch_size, num_classes, kernel_c, kernel_h, kernel_w).squeeze().to(windows.device)\n",
        "    repatch_com = torch.argmax(repatch_com,dim=1)               \n",
        "    del windows\n",
        "    gc.collect()\n",
        "    return repatch_com\n",
        "\n",
        "\n",
        "def cal_padding(n,k,stride,p):\n",
        "        #the two boundry: left  right\n",
        "        total_padding = stride - (n - (k+(int((n-k+2*p)/stride))*stride))\n",
        "        if total_padding == stride:\n",
        "          total_padding = 0                \n",
        "        left = int(total_padding/2)\n",
        "        right = total_padding - left\n",
        "        return left,right\n",
        "\n",
        "\n",
        "\n",
        "def patch_combine_one_by_one(data,num_image=1, C=96, H=96, W=96, kernel_c = 32, kernel_h=32, kernel_w = 32,step = 32 ):\n",
        "    \n",
        "    #downsampling size\n",
        "    batch_size, depth, height, width = num_image,img_size[2],img_size[0],img_size[1]\n",
        "    padding_left,padding_right = cal_padding(width,kernel_w,step,0)\n",
        "    padding_top,padding_bottom = cal_padding(height,kernel_h,step,0)\n",
        "    padding_front,padding_back = cal_padding(depth,kernel_c,step,0)\n",
        "    print(padding_left,padding_right,W,kernel_w,step)\n",
        "\n",
        "    recon_image_array =   torch.zeros((num_image,C,H,W),dtype = torch.int8).to(data.device)\n",
        "    interval = int(data.shape[0]/num_image)\n",
        "    for i in range(num_image):\n",
        "        recon_image = patch_combine(data[interval*(i):interval*(i+1)],1, C, H, W, kernel_c , kernel_h, kernel_w ,step )\n",
        "\n",
        "        recon_image_array[i] = recon_image\n",
        "    del recon_image\n",
        "    gc.collect()\n",
        "    # return recon_image_array\n",
        "    return recon_image_array[:,padding_front:depth+padding_front,padding_top:height+padding_top,padding_left:width+padding_left]\n",
        "\n",
        "# return reg after padding\n",
        "def load_reference_torch(idx):\n",
        "\n",
        "  print(idx)\n",
        "  seg_path = files_seg_seg_test[idx]\n",
        "  # msk_path = file_list_msk[idx]\n",
        "  # img = sitk.ReadImage(img_path, sitk.sitkFloat32)\n",
        "  seg = sitk.ReadImage(seg_path,sitk.sitkInt8)\n",
        "  # seg = sitk.GetArrayFromImage(seg)\n",
        "  seg = torch.from_numpy(sitk.GetArrayFromImage(seg))\n",
        "  # seg = prepadding(seg.unsqueeze(0),kernel_c,kernel_h,kernel_w,step)\n",
        "\n",
        "  \n",
        "  #display\n",
        "  seg_display = sitk.GetImageFromArray(seg.squeeze(0).cpu().numpy().astype(np.uint8))\n",
        "  seg_display = sitk.LabelToRGB(seg_display)\n",
        "  display_image(seg_display,figure_name = 'reference'+str(idx))\n",
        "  del seg_display\n",
        "\n",
        "  return seg.squeeze(0)\n",
        "\n",
        "\n",
        "def confusion_matrix_torch(img,seg):\n",
        "\n",
        "  # for class 0, TP\n",
        "  TP = torch.zeros(num_classes)\n",
        "  TN = torch.zeros(num_classes)\n",
        "  FN = torch.zeros(num_classes)\n",
        "  FP = torch.zeros(num_classes)\n",
        "\n",
        "  for i in range(num_classes):\n",
        "    img  =  img.to(device).char()\n",
        "    seg = seg.to(device)\n",
        "    tp =torch.sum(img[img == seg] == i)\n",
        "    TP[i] = tp\n",
        "    # print(tp)\n",
        "    #for class 0, FN\n",
        "    fn = torch.sum(img[seg == i] !=i)\n",
        "    # print(fn)\n",
        "    FN[i] = fn\n",
        "    #for class 0, FP\n",
        "    fp=torch.sum(seg[img == i] !=i)\n",
        "    FP[i] = fp\n",
        "    # print(fp)\n",
        "    #for class 0, TN\n",
        "    tn =torch.sum(img[seg != i] !=i)\n",
        "    TN[i] = tn\n",
        "    # print(tn)\n",
        "    \n",
        "  return TP,TN,FN,FP\n",
        "\n",
        "\n",
        "def dice_coefficient(TP,TN,FN,FP):\n",
        "    #   2 * TP / (FN + (2 * TP) + FP)\n",
        "  smooth = 1.\n",
        "  return 2 * TP / (FN + (2 * TP) + FP + smooth)\n",
        "\n",
        "def iou(TP,TN,FN,FP):\n",
        "  smooth = 1\n",
        "  return TP/(TP+FP+FN+smooth)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N99lefC2kjIR"
      },
      "source": [
        "\n",
        "def zero_mean_unit_var(image, mask):\n",
        "    \"\"\"Normalizes an image to zero mean and unit variance.\"\"\"\n",
        "\n",
        "    img_array = sitk.GetArrayFromImage(image)\n",
        "    img_array = img_array.astype(np.float32)\n",
        "\n",
        "    msk_array = sitk.GetArrayFromImage(mask)\n",
        "\n",
        "    mean = np.mean(img_array[msk_array>0])\n",
        "    std = np.std(img_array[msk_array>0])\n",
        "\n",
        "    if std > 0:\n",
        "        img_array = (img_array - mean) / std\n",
        "        img_array[msk_array==0] = 0\n",
        "\n",
        "    image_normalised = sitk.GetImageFromArray(img_array)\n",
        "    image_normalised.CopyInformation(image)\n",
        "\n",
        "    return image_normalised\n",
        "\n",
        "\n",
        "def resample_image(image, out_spacing=(1.0, 1.0, 1.0), out_size=None, is_label=False, pad_value=0):\n",
        "    \"\"\"Resamples an image to given element spacing and output size.\"\"\"\n",
        "\n",
        "    original_spacing = np.array(image.GetSpacing())\n",
        "    original_size = np.array(image.GetSize())\n",
        "\n",
        "    if out_size is None:\n",
        "        out_size = np.round(np.array(original_size * original_spacing / np.array(out_spacing))).astype(int)\n",
        "    else:\n",
        "        out_size = np.array(out_size)\n",
        "\n",
        "    original_direction = np.array(image.GetDirection()).reshape(len(original_spacing),-1)\n",
        "    original_center = (np.array(original_size, dtype=float) - 1.0) / 2.0 * original_spacing\n",
        "    out_center = (np.array(out_size, dtype=float) - 1.0) / 2.0 * np.array(out_spacing)\n",
        "\n",
        "    original_center = np.matmul(original_direction, original_center)\n",
        "    out_center = np.matmul(original_direction, out_center)\n",
        "    out_origin = np.array(image.GetOrigin()) + (original_center - out_center)\n",
        "\n",
        "    resample = sitk.ResampleImageFilter()\n",
        "    resample.SetOutputSpacing(out_spacing)\n",
        "    resample.SetSize(out_size.tolist())\n",
        "    resample.SetOutputDirection(image.GetDirection())\n",
        "    resample.SetOutputOrigin(out_origin.tolist())\n",
        "    resample.SetTransform(sitk.Transform())\n",
        "    resample.SetDefaultPixelValue(pad_value)\n",
        "\n",
        "    if is_label:\n",
        "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
        "    else:\n",
        "        resample.SetInterpolator(sitk.sitkBSpline)\n",
        "    import os\n",
        "    import sys\n",
        "    import psutil\n",
        "\n",
        "    del original_size\n",
        "\n",
        "    gc.collect()\n",
        "    return resample.Execute(image)\n",
        "\n",
        "def prepadding(data,kernel_c,kernel_h,kernel_w,step):\n",
        "    #batch_size, depth(n_channels), n_rows, n_cols\n",
        "    import torch\n",
        "    import torch.nn\n",
        "    batch_size, depth, height, width = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unsqueeze(1)\n",
        "\n",
        "    # calculate the padding for two edges\n",
        "    def cal_padding(n,k,stride,p):\n",
        "        #the two boundry: left  right\n",
        "        total_padding = stride - (n - (k+(int((n-k+2*p)/stride))*stride))\n",
        "        if total_padding == stride:\n",
        "          total_padding = 0\n",
        "        left = int(total_padding/2)\n",
        "        right = total_padding - left\n",
        "        return left,right\n",
        "\n",
        "    padding_left,padding_right = cal_padding(n=width,k=kernel_w,stride=step,p=0)\n",
        "    padding_top,padding_bottom = cal_padding(n=height,k=kernel_h,stride=step,p=0)\n",
        "    padding_front,padding_back = cal_padding(n=depth,k=kernel_c,stride=step,p=0)\n",
        "\n",
        "    output = torch.zeros((batch_size,depth+padding_front+padding_back,height+padding_top+padding_bottom,width+padding_left+padding_right),dtype = data.dtype).to(device)\n",
        "    output[:,padding_front:depth+padding_front,padding_top:height+padding_top,padding_left:width+padding_left]= data\n",
        "    print(output.shape)\n",
        "    del data\n",
        "    gc.collect()\n",
        "    return output\n",
        "\n",
        "\n",
        "def get_patches(data,kernel_c=32, kernel_h=32, kernel_w=32,step=32):\n",
        "    data = prepadding(data,kernel_c,kernel_h,kernel_w,step)\n",
        "    gc.collect()\n",
        "    batch_size, n_channels, n_rows, n_cols = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unfold(1, kernel_c, step).unfold(2, kernel_h, step).unfold(3, kernel_w, step)\n",
        "    data = data.permute(1, 2, 3, 0, 4, 5, 6).reshape(-1, kernel_c, kernel_h, kernel_w)\n",
        "    \n",
        "    return data\n",
        "\n",
        "\n",
        "def get_patches_stage1(data,kernel_c=32, kernel_h=32, kernel_w=32,step=32):\n",
        "    \n",
        "    data = prepadding(data,kernel_c,kernel_h,kernel_w,step)\n",
        "    \n",
        "    gc.collect()\n",
        "    batch_size, n_channels, n_rows, n_cols = data.shape[0],data.shape[1],data.shape[2],data.shape[3]\n",
        "    data = data.unfold(1, kernel_c, step).unfold(2, kernel_h, step).unfold(3, kernel_w, step)\n",
        "    data = data.permute(1, 2, 3, 0, 4, 5, 6).reshape(-1, kernel_c, kernel_h, kernel_w)\n",
        "    data = make_one_hot(data.unsqueeze(1),[x for x in range(num_classes)]) #num, channel(3), D,H,W\n",
        "    print('stage1 seg shape',data.shape)\n",
        "    return data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRX6y9XykjIS"
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "class ImageSegmentationDataset_stage2(Dataset):\n",
        "    \"\"\"Dataset for image segmentation.\"\"\"\n",
        "\n",
        "    def __init__(self, stage1_prediction, file_list_img, file_list_seg, img_spacing, img_size,\n",
        "                 patch=True, kernel_c=64, kernel_h=64, kernel_w=64, step=32, TEST=False):\n",
        "        self.imgs = []\n",
        "        self.segs = []\n",
        "        self.img_names = []\n",
        "        self.seg_names = []\n",
        "        # count the number of class items in patches for weights\n",
        "        self.counts = torch.zeros(9)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            for idx, _ in enumerate(tqdm(range(len(file_list_img)), desc='Loading Data')):\n",
        "                valid_index = []\n",
        "                seg_path = file_list_seg[idx]\n",
        "\n",
        "                seg = sitk.ReadImage(seg_path, sitk.sitkInt8)\n",
        "\n",
        "                seg = torch.from_numpy(sitk.GetArrayFromImage(seg)).unsqueeze(0).to(device)\n",
        "\n",
        "                # patches\n",
        "                print('seg.dtype',seg.dtype)\n",
        "                print('seg.dtype',seg.shape)\n",
        "                seg = get_patches(seg, kernel_c, kernel_h, kernel_w, step)\n",
        "\n",
        "                for index_patches in range(seg.shape[0]):\n",
        "\n",
        "                    sum_0 = torch.sum(seg[index_patches] == 0)\n",
        "                    sum_numel = seg[index_patches].numel()\n",
        "                    background = sum_0 / sum_numel\n",
        "                    print(sum_0, sum_numel, background)\n",
        "\n",
        "\n",
        "                    if background >= threshold and not TEST:\n",
        "                      if torch.rand(1)<roulette:\n",
        "                        pass\n",
        "                      else:\n",
        "                        continue\n",
        "\n",
        "                    uni = [x for x in range(num_classes)]\n",
        "                    for i in uni:\n",
        "                      sum_item = torch.sum(seg[index_patches] == i)\n",
        "                      self.counts[i] += sum_item.cpu()\n",
        "\n",
        "                  \n",
        "\n",
        "                    valid_index.append(index_patches)\n",
        "                    sample =  seg[index_patches].unsqueeze(0).cpu()\n",
        "                    self.segs.append(sample)\n",
        "\n",
        "                    self.seg_names.append(os.path.basename(seg_path))\n",
        "                    import gc\n",
        "                    del sample\n",
        "                    gc.collect()\n",
        "                del seg;\n",
        "                import gc\n",
        "                gc.collect()\n",
        "\n",
        "\n",
        "                ###img\n",
        "                img_path = file_list_img[idx]\n",
        "                img = sitk.ReadImage(img_path, sitk.sitkUInt8)\n",
        "                   \n",
        "            \n",
        "                # from image  numpy to torch\n",
        "                img = torch.from_numpy(sitk.GetArrayFromImage(img)).unsqueeze(0).to(device)\n",
        "\n",
        "                # patches\n",
        "                torch.cuda.empty_cache()\n",
        "                img = get_patches(img, kernel_c, kernel_h, kernel_w, step).unsqueeze(1) \n",
        "\n",
        "                stage1_seg = get_patches(stage1_prediction[idx].to(device),kernel_c, kernel_h, kernel_w, step) \n",
        "\n",
        "                for index_patches in range(img.shape[0]):\n",
        "                    if index_patches  not in valid_index:\n",
        "                        continue\n",
        "\n",
        "\n",
        "                    sample = img[index_patches]\n",
        "                    stage1_sample = stage1_seg[index_patches]\n",
        "\n",
        "                    sample = torch.cat((sample,stage1_sample.unsqueeze(0)),0).byte()\n",
        "                    print('sample.shape:',sample.shape)\n",
        "                    del stage1_sample\n",
        "                    \n",
        "                    self.imgs.append(sample.cpu())\n",
        "                 \n",
        "                    self.img_names.append(os.path.basename(img_path))\n",
        "                    import gc\n",
        "                    del sample\n",
        "                    gc.collect()\n",
        "                del img;\n",
        "                del stage1_seg\n",
        "                gc.collect()\n",
        "            \n",
        "            \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        img = self.imgs[item]\n",
        "        seg = self.segs[item]\n",
        "        return {'img': img, 'seg': seg}\n",
        "\n",
        "    def get_sample(self, item):\n",
        "        img = self.imgs[item]\n",
        "        seg = self.segs[item]\n",
        "        return {'img': img, 'seg': seg}\n",
        "\n",
        "    def get_img_name(self, item):\n",
        "        return self.img_names[item]\n",
        "\n",
        "    def get_seg_name(self, item):\n",
        "        return self.seg_names[item]\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qp6q4wZnkjIS"
      },
      "source": [
        "##metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgDn7eC7kovj"
      },
      "source": [
        "dataset_test = ImageSegmentationDataset_separate(files_seg_img_test, files_seg_seg_test, img_spacing, img_size, True,kernel_c,kernel_h,kernel_w,step, True)\n",
        "dataloader_testall = []\n",
        "for index in range(len(files_seg_img_test)):\n",
        "  dataloader_test = torch.utils.data.DataLoader(torch.utils.data.Subset(dataset_test,list(range(int(num_patch_per_block*index),int(num_patch_per_block*(index+1)), 1))), batch_size=1, shuffle=False, num_workers = 2, prefetch_factor=16,pin_memory=True)\n",
        "  dataloader_testall.append(dataloader_test)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z5B7FjT1RUt"
      },
      "source": [
        "uni_labels = [x for x in range(num_classes)]\n",
        "vs_scores = torch.zeros(len(files_seg_img_test),len(uni_labels))\n",
        "TP = torch.zeros((vs_scores.shape[0],num_classes))\n",
        "TN = torch.zeros((vs_scores.shape[0],num_classes))\n",
        "FN = torch.zeros((vs_scores.shape[0],num_classes))\n",
        "FP = torch.zeros((vs_scores.shape[0],num_classes))\n",
        "\n",
        "\n",
        "for index in range(len(files_seg_img_test)):\n",
        "\n",
        "  num_image = 1\n",
        "  model_dir = os.path.join(out_dir, 'model')\n",
        "  uni_labels = [x for x in range(num_classes)]\n",
        "  print(num_patch_per_block)\n",
        "  print(step)\n",
        "  #start testing\n",
        "\n",
        "  patch_store = torch.zeros(num_image*num_patch_per_block,kernel_c,kernel_h,kernel_w,dtype = torch.int8).to(device)\n",
        "\n",
        "\n",
        "  pred_dir = os.path.join(out_dir, 'pred')\n",
        "  if not os.path.exists(pred_dir):\n",
        "      os.makedirs(pred_dir)\n",
        "\n",
        "  model = UNet3d()\n",
        "\n",
        "  PATH = save_path = os.path.join(model_dir, 'model.pt')\n",
        "\n",
        "\n",
        "  if device  == torch.device(\"cpu\"):\n",
        "    checkpoint = torch.load(PATH,map_location=torch.device('cpu'))\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  else:\n",
        "    checkpoint = torch.load(PATH)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "      \n",
        "  print('START TESTING...')\n",
        "\n",
        "  loss_test = 0\n",
        "  sum_pts = 0\n",
        "  idx_test = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "      for index_patch,data_sample in enumerate(dataloader_testall[index]):\n",
        "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
        "          img, seg = data_sample['img'].to(device).half(), data_sample['seg'].to(device)\n",
        "          img_global = data_sample['img_global'].to(device).half()\n",
        "          prd = model(img,img_global)\n",
        "          prd = torch.argmax(prd[0], dim=1)\n",
        "\n",
        "          # print(index_patch)\n",
        "\n",
        "          patch_store[index_patch] = prd\n",
        "          \n",
        "  del prd;gc.collect()\n",
        "  print('\\nFinished TESTING.')\n",
        "  print(C,H,W)\n",
        "\n",
        "  #from patch to whole data\n",
        "  recon_image_original = patch_combine_one_by_one(patch_store, num_image, C_,H_,W_, kernel_c, kernel_h, kernel_w,step).cpu()\n",
        "\n",
        "\n",
        "  #evaluation\n",
        "  \n",
        "  #input: recon_image_original   \n",
        "  for idx in range(num_image):\n",
        "\n",
        "    # print('i')\n",
        "    tp,tn,fn,fp = 0,0,0,0 \n",
        "    \n",
        "    #original label \n",
        "    seg_torch = load_reference_torch(index)\n",
        "    seg_shape = seg_torch.shape\n",
        "    #shape for one image\n",
        "    C,H,W = seg_shape\n",
        "    # print('index {},c {},h {},w {}'.format(index,C,H,W))\n",
        "    seg_recon = recon_image_original[idx]\n",
        "\n",
        "    # display\n",
        "    prediction = sitk.GetImageFromArray(seg_recon.numpy().astype(np.uint8))\n",
        "    prediction2 = sitk.LabelToRGB(prediction)\n",
        "    display_image(prediction2,figure_name ='prediction'+str(index))  \n",
        "    \n",
        "    #overlay\n",
        "    green = [0,255,0] ;gold = [255,215,0];colo =[238, 64, 0]\n",
        "    image_test = load_original_image_torch(index)\n",
        "    overlay_prd = sitk.LabelOverlay(image_test,prediction,colormap=gold+green+colo,opacity=0.55)\n",
        "    display_image(overlay_prd,figure_name ='overlay_prediction'+str(index))  \n",
        "    del prediction\n",
        "    del prediction2\n",
        "    del image_test\n",
        "    del overlay_prd\n",
        "    gc.collect()\n",
        "\n",
        "    #prediction\n",
        "    img = seg_recon.squeeze().flatten()\n",
        "\n",
        "    #label\n",
        "    seg = seg_torch.squeeze().flatten()\n",
        "\n",
        "    tp,tn,fn,fp = confusion_matrix_torch(img,seg)\n",
        "    vs = 1-abs(fn-fp)/(2*tp+fp+fn+1e-9)\n",
        "    #make one hot\n",
        "    img = make_one_hot(seg_recon.unsqueeze(0).unsqueeze(0),[x for x in range(num_classes)])\n",
        "    seg = make_one_hot(seg_torch.unsqueeze(0).unsqueeze(0),[x for x in range(num_classes)])\n",
        "\n",
        "    TP[index] = (tp)\n",
        "    TN[index] = (tn)\n",
        "    FN[index] = (fn)\n",
        "    FP[index] = (fp)\n",
        "    del seg_torch\n",
        "    del img;del seg;del seg_recon;gc.collect()\n",
        "\n",
        "  del patch_store\n",
        "  del recon_image_original\n",
        "  gc.collect()\n",
        "\n",
        "\n",
        "\n",
        "iou_score = iou(TP,TN,FN,FP)\n",
        "dice_scores = dice_coefficient(TP,TN,FN,FP)\n",
        "print('dice_scores:',dice_scores)\n",
        "print('IoU:', iou_score)\n",
        "\n",
        "dice_std = torch.std(dice_scores, dim=0)\n",
        "dice_mean = torch.mean(dice_scores,dim = 0)\n",
        "IoU_std = torch.std(iou_score,dim=0)\n",
        "IoU_mean = torch.mean(iou_score,dim=0)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGZRRQY7wQ63"
      },
      "source": [
        "print('dice_std',dice_std)\n",
        "print('dice_mean',dice_mean)\n",
        "print('IoU_std',IoU_std)\n",
        "print('IoU_mean',IoU_mean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpto3VzCkjIS"
      },
      "source": [
        "<!-- torch.Size([1, 1, 160, 160, 160]) torch.Size([1, 1, 160, 160, 160])\n",
        "dice_scores: tensor([[0.9998, 0.7049, 0.6927],\n",
        "        [0.9998, 0.8169, 0.5936],\n",
        "        [0.9991, 0.9381, 0.7051],\n",
        "        [0.9988, 0.9097, 0.8005],\n",
        "        [0.9998, 0.9486, 0.5345],\n",
        "        [0.9998, 0.9640, 0.8184]])\n",
        "IoU: tensor([[0.9995, 0.5442, 0.5298],\n",
        "        [0.9997, 0.6905, 0.4220],\n",
        "        [0.9983, 0.8835, 0.5445],\n",
        "        [0.9976, 0.8343, 0.6674],\n",
        "        [0.9997, 0.9021, 0.3647],\n",
        "        [0.9996, 0.9306, 0.6926]])\n",
        "dice_std tensor([0.0004, 0.1007, 0.1117])\n",
        "dice_mean tensor([0.9995, 0.8804, 0.6908])\n",
        "IoU_std tensor([0.0009, 0.1503, 0.1298])\n",
        "IoU_mean tensor([0.9991, 0.7975, 0.5368]) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e3hwSaxxuYk"
      },
      "source": [
        "<!-- torch.Size([1, 1, 160, 160, 160]) torch.Size([1, 1, 160, 160, 160])\n",
        "dice_scores: tensor([[0.9998, 0.7049, 0.6927],\n",
        "        [0.9998, 0.8169, 0.5936],\n",
        "        [0.9991, 0.9381, 0.7051],\n",
        "        [0.9988, 0.9097, 0.8005],\n",
        "        [0.9998, 0.9486, 0.5345],\n",
        "        [0.9998, 0.9640, 0.8184]])\n",
        "IoU: tensor([[0.9995, 0.5442, 0.5298],\n",
        "        [0.9997, 0.6905, 0.4220],\n",
        "        [0.9983, 0.8835, 0.5445],\n",
        "        [0.9976, 0.8343, 0.6674],\n",
        "        [0.9997, 0.9021, 0.3647],\n",
        "        [0.9996, 0.9306, 0.6926]])\n",
        "dice_std tensor([0.0004, 0.1007, 0.1117])\n",
        "dice_mean tensor([0.9995, 0.8804, 0.6908])\n",
        "IoU_std tensor([0.0009, 0.1503, 0.1298])\n",
        "IoU_mean tensor([0.9991, 0.7975, 0.5368]) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S11PX9n4Up3B"
      },
      "source": [
        "<!-- torch.Size([1, 1, 160, 160, 160]) torch.Size([1, 1, 160, 160, 160])\n",
        "dice_scores: tensor([[0.9998, 0.7049, 0.6927],\n",
        "        [0.9998, 0.8169, 0.5936],\n",
        "        [0.9991, 0.9381, 0.7051],\n",
        "        [0.9988, 0.9097, 0.8005],\n",
        "        [0.9998, 0.9486, 0.5345],\n",
        "        [0.9998, 0.9640, 0.8184]])\n",
        "IoU: tensor([[0.9995, 0.5442, 0.5298],\n",
        "        [0.9997, 0.6905, 0.4220],\n",
        "        [0.9983, 0.8835, 0.5445],\n",
        "        [0.9976, 0.8343, 0.6674],\n",
        "        [0.9997, 0.9021, 0.3647],\n",
        "        [0.9996, 0.9306, 0.6926]])\n",
        "dice_std tensor([0.0004, 0.1007, 0.1117])\n",
        "dice_mean tensor([0.9995, 0.8804, 0.6908])\n",
        "IoU_std tensor([0.0009, 0.1503, 0.1298])\n",
        "IoU_mean tensor([0.9991, 0.7975, 0.5368]) -->"
      ]
    }
  ]
}